{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 更改的部分\n",
    "\n",
    "* pose estimation 姿態辨識改為使用paddlehub提供的 `human_pose_estimation_resnet50_mpii`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective:\n",
    "\n",
    "As input to the system, take the live feed from the webcam and use pose estimation to map out a small dance tutorial.\n",
    "\n",
    "# Approach:\n",
    "- We will take a pretrained **openpose estimation model** to prdict the **18 keypoints** on a human body.\n",
    "- We take openpose model for tensorflow by Ildoo Kim\n",
    "  - GitHub Repo Link: https://github.com/ildoonet/tf-pose-estimation\n",
    "<br>**[!] Note**: Some how I found issues with this repo to work with tensorflow 2.0 and followed a modified repo of his by Gunjan Seth.<br>\n",
    "GitHub Repo Link: https://github.com/gsethi2409/tf-pose-estimation\n",
    "<br>Medium Blog by Gunjan Seth: https://medium.com/@gsethi2409/pose-estimation-with-tensorflow-2-0-a51162c095ba\n",
    "- The keypoints of the dancer are obtained and stored in a array list.\n",
    "- These keypoints are **normalized**.\n",
    "- The user feed is taken and the keypoints are detected.\n",
    "- The keypoints are normalized and the **cosine similarity** is found between the user keypoints and the array of dancer keypoints.\n",
    "- The minimum similarity score is **compared with the threshold** and then it displays is the user steps are correct or not for the given dancer moves.\n",
    "\n",
    "# Constraints To Look For:\n",
    "1. The model should be fast for prediction. Latency should be avoided.\n",
    "2. Predictions should be accurate and the steps should be close enough with  the dancer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python = 3.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlepaddle\n",
    "!pip install --upgrade paddlehub\n",
    "!pip install moviepy\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/anaconda3/envs/CV_Project/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/brian/anaconda3/envs/CV_Project/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "shm_open() failed: No such file or directory\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5180:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5180:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5180:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5703:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2666:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "shm_open() failed: No such file or directory\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5180:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5180:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5180:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5703:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2666:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "import os\n",
    "import cv2\n",
    "import paddlehub as hub\n",
    "from moviepy.editor import *\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg \n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hub install human_pose_estimation_resnet50_mpii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take position from the trainer (dancer):\n",
    "- We made two functions to get all the keypoints from the trainer and store them in a dataframe and in a list.\n",
    "-  The function **\"dance_video_processing\"** is used to predict all the keypoints from the video and return all the keypoints for the video.\n",
    "- The function **\"get_position\"** is used to take all the keypoints that are returned from the above function, preprocess them and return the dataframe and the list of keypoints.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`human_pose_estimation_resnet50_mpii` 模型可以檢測 16 個關鍵點。這些關鍵點分別是：\n",
    "\n",
    "1. 頭頂 (head_top)\n",
    "2. 頸部 (neck)\n",
    "3. 右肩 (right_shoulder)\n",
    "4. 右肘 (right_elbow)\n",
    "5. 右腕 (right_wrist)\n",
    "6. 左肩 (left_shoulder)\n",
    "7. 左肘 (left_elbow)\n",
    "8. 左腕 (left_wrist)\n",
    "9. 右髋 (right_hip)\n",
    "10. 右膝 (right_knee)\n",
    "11. 右踝 (right_ankle)\n",
    "12. 左髋 (left_hip)\n",
    "13. 左膝 (left_knee)\n",
    "14. 左踝 (left_ankle)\n",
    "15. 胸部 (thorax)\n",
    "16. 骶骨 (pelvis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_dimensions(video_path):\n",
    "    \"\"\"取得影片的解析度\n",
    "\n",
    "    Args:\n",
    "        video_path (string) : 影片路徑\n",
    "\n",
    "    Returns:\n",
    "        tuple (width, height) : 影片的解析度\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video stream or file\")\n",
    "        return None\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    cap.release()\n",
    "    return (width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影片尺寸: 寬度=406, 高度=718\n"
     ]
    }
   ],
   "source": [
    "# 示例使用\n",
    "video_path = r'dance_video/dancer.mp4'\n",
    "dimensions = get_video_dimensions(video_path)\n",
    "if dimensions:\n",
    "    print(f\"影片尺寸: 寬度={dimensions[0]}, 高度={dimensions[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加載新的人體姿勢估計模型\n",
    "pose_resnet50_mpii = hub.Module(name=\"human_pose_estimation_resnet50_mpii\")\n",
    "\n",
    "\n",
    "\n",
    "def dance_video_processing(video_path, showBG=True, dim = (640, 480)):\n",
    "    \"\"\"\n",
    "    對輸入的視頻進行處理，提取每一幀的關鍵點信息。\n",
    "\n",
    "    Args:\n",
    "        video_path (str): 視頻文件的路徑。\n",
    "        showBG (bool, optional): 是否顯示背景。默認為 True。\n",
    "        dim (tuple, optional): 調整圖像的尺寸。默認為 (640, 480)。\n",
    "\n",
    "    Returns:\n",
    "        list: 包含每幀圖像中提取的16個關鍵點的(x,y)坐標信息。\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video stream or file\")\n",
    "        return\n",
    "\n",
    "    fps_time = 0\n",
    "    keypoints_list = []\n",
    "\n",
    "    while True:\n",
    "        ret_val, image = cap.read()\n",
    "        if not ret_val:\n",
    "            break\n",
    "\n",
    "        # resize image\n",
    "        image = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)\n",
    "        results = pose_resnet50_mpii.keypoint_detection(images=[image], use_gpu=False)\n",
    "\n",
    "        if not showBG:\n",
    "            image = np.zeros(image.shape)\n",
    "\n",
    "        if results and len(results) > 0:\n",
    "            keypoints = results[0]['data']\n",
    "            keypoints_list.append(keypoints)\n",
    "            for key, (x, y) in keypoints.items():\n",
    "                if x > 0 and y > 0:\n",
    "                    cv2.circle(image, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "                    cv2.putText(image, f'{key}', (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "        # To display fps\n",
    "        cv2.putText(image, \"FPS: %f\" % (1.0 / (time.time() - fps_time)), (10, 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        # To display image\n",
    "        cv2.imshow('Dancer', image)\n",
    "        fps_time = time.time()\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return keypoints_list\n",
    "\n",
    "\n",
    "\n",
    "def get_position(video_path, showBG=True, dim = (640, 480)):\n",
    "    \"\"\"\n",
    "    提取視頻中的關鍵點，將關鍵點座標(x,y)分開儲存在dataframe中。\n",
    "\n",
    "    Args:\n",
    "        video_path (str): 視頻文件的路徑。\n",
    "        showBG (bool, optional): 是否顯示背景。默認為 True。\n",
    "        dim (tuple, optional): 調整圖像的尺寸。默認為 (640, 480)。\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: 包含每幀圖像中提取的關鍵點信息的數據框。\n",
    "        list: 包含每幀圖像中提取的關鍵點信息的列表。\n",
    "    \"\"\"\n",
    "    keypoints_list = dance_video_processing(video_path, showBG, dim)\n",
    "\n",
    "    # 初始化特徵列表\n",
    "    features = [0] * 32\n",
    "    keyp_list = []\n",
    "\n",
    "    # 預處理關鍵點數據\n",
    "    for keypoints in keypoints_list:\n",
    "        features = [0] * 32\n",
    "        for idx, part in enumerate(['left_ankle', 'left_knee', 'left_hip', 'right_hip', 'right_knee', 'right_ankle', 'pelvis', 'thorax', 'upper_neck', 'head_top', 'right_wrist', 'right_elbow', 'right_shoulder', 'left_shoulder', 'left_elbow', 'left_wrist']):\n",
    "            if part in keypoints:\n",
    "                x, y = keypoints[part]\n",
    "                features[idx * 2] = x\n",
    "                features[idx * 2 + 1] = y\n",
    "        keyp_list.append(features)\n",
    "\n",
    "    # 構建數據框的列名\n",
    "    column_names = [str(i) for i in range(32)]\n",
    "    data = pd.DataFrame(keyp_list, columns=column_names)\n",
    "\n",
    "    return data, keyp_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_skeleton(image, keypoints, color=(0, 255, 0), thickness=2):\n",
    "    \"\"\"\n",
    "    在圖像上畫出骨架。\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): 圖像。\n",
    "        keypoints (dict): 關鍵點字典，包含每個關鍵點的座標。\n",
    "        color (tuple, optional): 骨架的顏色。默認為綠色。\n",
    "        thickness (int, optional): 骨架的線條粗細。默認為2。\n",
    "    \"\"\"\n",
    "    skeleton = [\n",
    "        ('head_top', 'upper_neck'),\n",
    "        ('upper_neck', 'thorax'),\n",
    "        ('upper_neck', 'left_shoulder'),\n",
    "        ('upper_neck', 'right_shoulder'),\n",
    "        ('left_shoulder', 'left_elbow'),\n",
    "        ('left_elbow', 'left_wrist'),\n",
    "        ('right_shoulder', 'right_elbow'),\n",
    "        ('right_elbow', 'right_wrist'),\n",
    "        ('thorax', 'left_hip'),\n",
    "        ('left_hip', 'left_knee'),\n",
    "        ('left_knee', 'left_ankle'),\n",
    "        ('thorax', 'right_hip'),\n",
    "        ('right_hip', 'right_knee'),\n",
    "        ('right_knee', 'right_ankle')\n",
    "    ]\n",
    "    \n",
    "    for joint1, joint2 in skeleton:\n",
    "        if joint1 in keypoints and joint2 in keypoints:\n",
    "            point1 = keypoints[joint1]\n",
    "            point2 = keypoints[joint2]\n",
    "            if point1[0] > 0 and point1[1] > 0 and point2[0] > 0 and point2[1] > 0:\n",
    "                cv2.line(image, (point1[0], point1[1]), (point2[0], point2[1]), color, thickness)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Observation:** \n",
    "- We can see how the keypoints data looks from the above example.\n",
    "- Since they are 16 keypoints and each keypoint has **x-coordinate** and **y-coordinate** we have **32 columns** (16 x 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相似度計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity:\n",
    "Cosine Similarity function for our model to find the keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def findCosineSimilarity_1(source_representation, test_representation):\n",
    "    \"\"\"\n",
    "    計算兩個向量之間的餘弦相似度。\n",
    "    餘弦相似度越低，表示兩個向量越相似。\n",
    "\n",
    "    Args:\n",
    "        source_representation (numpy.ndarray): 來源向量。\n",
    "        test_representation (numpy.ndarray): 測試向量。\n",
    "\n",
    "    Returns:\n",
    "        float: 餘弦相似度，範圍在 [0, 1] 之間。值越低表示相似度越高。\n",
    "    \"\"\"\n",
    "    # 計算內積\n",
    "    a = np.matmul(np.transpose(source_representation), test_representation)\n",
    "    # 計算每個向量的范數\n",
    "    b = np.sum(np.multiply(source_representation, source_representation))\n",
    "    c = np.sum(np.multiply(test_representation, test_representation))\n",
    "    # 返回餘弦相似度\n",
    "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing:\n",
    "Comparing the user images with keypoints of the dancer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_positions(trainer_video, user_video, keyp_list, dim=(640, 480)):\n",
    "    \"\"\"\n",
    "    比較兩個視頻中的姿勢，並在視頻中畫出骨架。\n",
    "\n",
    "    Args:\n",
    "        trainer_video (str): 舞者視頻文件的路徑。\n",
    "        user_video (str): 用戶視頻文件的路徑。\n",
    "        keyp_list (list): 舞者關鍵點列表。\n",
    "        dim (tuple, optional): 調整圖像的尺寸。默認為 (640, 480)。\n",
    "    \"\"\"\n",
    "    # 打開視頻文件\n",
    "    cap = cv2.VideoCapture(trainer_video)\n",
    "    cam = cv2.VideoCapture(user_video)\n",
    "    \n",
    "    # 檢查視頻文件是否成功打開\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Unable to open trainer video.\")\n",
    "        return\n",
    "    if not cam.isOpened():\n",
    "        print(\"Error: Unable to open user video.\")\n",
    "        return\n",
    "    \n",
    "    # 設置用戶視頻的尺寸\n",
    "    cam.set(3, dim[0])\n",
    "    cam.set(4, dim[1])\n",
    "    fps_time = 0\n",
    "\n",
    "    while True:\n",
    "        # 讀取視頻幀\n",
    "        ret_val, image_1 = cam.read()\n",
    "        ret_val_1, image_2 = cap.read()\n",
    "        \n",
    "        if not ret_val or not ret_val_1:\n",
    "            print(\"Error: Unable to read frame from video.\")\n",
    "            break\n",
    "        \n",
    "        # 調整圖像尺寸\n",
    "        image_2 = cv2.resize(image_2, dim, interpolation=cv2.INTER_AREA)\n",
    "        image_1 = cv2.resize(image_1, dim, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # 獲取舞者的關鍵點\n",
    "        trainer_results = pose_resnet50_mpii.keypoint_detection(images=[image_2], use_gpu=False)\n",
    "        if trainer_results and len(trainer_results) > 0:\n",
    "            trainer_keypoints = trainer_results[0]['data']\n",
    "            trainer_features = [0] * 32\n",
    "            # 將關鍵點轉換為特徵向量\n",
    "            for idx, key in enumerate(['head_top', 'neck', 'right_shoulder', 'right_elbow', 'right_wrist', 'left_shoulder', 'left_elbow', 'left_wrist', 'right_hip', 'right_knee', 'right_ankle', 'left_hip', 'left_knee', 'left_ankle', 'thorax', 'pelvis']):\n",
    "                if key in trainer_keypoints:\n",
    "                    x, y = trainer_keypoints[key]\n",
    "                    trainer_features[idx * 2] = x\n",
    "                    trainer_features[idx * 2 + 1] = y\n",
    "            # 標準化特徵向量\n",
    "            transformer = Normalizer().fit([trainer_features])\n",
    "            keyp_list = transformer.transform(keyp_list)\n",
    "            # 畫出舞者骨架\n",
    "            image_2 = draw_skeleton(image_2, trainer_keypoints)\n",
    "        else:\n",
    "            print(\"Error: No keypoints detected for the trainer video.\")\n",
    "            break\n",
    "\n",
    "        # 獲取用戶的關鍵點\n",
    "        user_results = pose_resnet50_mpii.keypoint_detection(images=[image_1], use_gpu=False)\n",
    "        if user_results and len(user_results) > 0:\n",
    "            user_keypoints = user_results[0]['data']\n",
    "            user_features = [0] * 32\n",
    "            # 將關鍵點轉換為特徵向量\n",
    "            for idx, key in enumerate(['head_top', 'neck', 'right_shoulder', 'right_elbow', 'right_wrist', 'left_shoulder', 'left_elbow', 'left_wrist', 'right_hip', 'right_knee', 'right_ankle', 'left_hip', 'left_knee', 'left_ankle', 'thorax', 'pelvis']):\n",
    "                if key in user_keypoints:\n",
    "                    x, y = user_keypoints[key]\n",
    "                    user_features[idx * 2] = x\n",
    "                    user_features[idx * 2 + 1] = y\n",
    "            # 標準化用戶特徵向量\n",
    "            user_features = transformer.transform([user_features])\n",
    "\n",
    "            # 計算用戶特徵向量與舞者特徵向量之間的餘弦相似度\n",
    "            min_similarity = 100\n",
    "            for kp in keyp_list:\n",
    "                sim_score = findCosineSimilarity_1(kp, user_features[0])\n",
    "                if min_similarity > sim_score:\n",
    "                    min_similarity = sim_score\n",
    "\n",
    "            # 在用戶視頻上顯示餘弦相似度\n",
    "            cv2.putText(image_1, str(min_similarity), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "            # 根據餘弦相似度判斷動作是否正確\n",
    "            if min_similarity < 0.2:\n",
    "                cv2.putText(image_1, \"CORRECT STEPS\", (120, 700), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.putText(image_1, \"NOT CORRECT STEPS\", (80, 700), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            \n",
    "            # 畫出用戶骨架\n",
    "            image_1 = draw_skeleton(image_1, user_keypoints)\n",
    "        else:\n",
    "            print(\"Error: No keypoints detected for the user video.\")\n",
    "            break\n",
    "\n",
    "        # 在舞者視頻上顯示 FPS\n",
    "        cv2.putText(image_2, \"FPS: %f\" % (1.0 / (time.time() - fps_time)), (10, 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        # 在用戶視頻上顯示 FPS\n",
    "        cv2.putText(image_1, \"FPS: %f\" % (1.0 / (time.time() - fps_time)), (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # 顯示視頻幀\n",
    "        cv2.imshow('Dancer Window', image_2)\n",
    "        cv2.imshow('User Window', image_1)\n",
    "\n",
    "        fps_time = time.time()\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # 釋放視頻對象\n",
    "    cam.release()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例调用\n",
    "trainer_video = 'dance_video/dancer.mp4'\n",
    "dimensions = get_video_dimensions(trainer_video)\n",
    "data, keyp_list = get_position(trainer_video, showBG=True, dim = (dimensions[0], dimensions[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例调用\n",
    "trainer_video = 'dance_video/breaking.mp4'\n",
    "dimensions = get_video_dimensions(trainer_video)\n",
    "data, keyp_list = get_position(trainer_video, showBG=True, dim = (dimensions[0], dimensions[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note:\n",
    "Since I cant dance, I'll be using a video for this :P.<br> We can replce the **user_video** attribute to **0 or 1** to turn on live camera depending on the type of camera we have.\n",
    "### For a wrong positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_video = 'dance_video/dancer.mp4'\n",
    "dimensions = get_video_dimensions(trainer_video)\n",
    "compare_positions(trainer_video, 'dance_video/locking.mp4', keyp_list, dim=(dimensions[0], dimensions[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For a correct positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_video = 'dance_video/dancer.mp4'\n",
    "dimensions = get_video_dimensions(trainer_video)\n",
    "compare_positions('dance_video/dancer.mp4','dance_video/right_dance.mp4',keyp_list, dim=(dimensions[0], dimensions[1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the same video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_video = 'dance_video/dancer.mp4'\n",
    "dimensions = get_video_dimensions(trainer_video)\n",
    "compare_positions('dance_video/dancer.mp4','dance_video/dancer.mp4',keyp_list, dim = (dimensions[0], dimensions[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have developed a pose estimation similarity pipeline to compare similarity between two poses from the given feed of videos or live cam.<br>\n",
    "**Flaws:**\n",
    "- This approach fails when the trainer is far or the user is near to the camera or vise-versa. This happens because there is a **scale variation** between the keypoints of the image.<br>\n",
    "**Solution:**\n",
    "- We can eleminate this problem by **croping out the image of a peron** using a CNN architecture like Yolo or anything that could detect the bounding boxes of a person.\n",
    "- This image then can be fed to the openpose model to estimate keypoints for both the sources.<br>\n",
    "**Scope of improvement:**\n",
    "- The accuracy of the model for keypoint prediction can be increased by taking a much powerful pretrained model architecture than mobilenet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加入 YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yolo_names_path = \"coco.names\"\n",
    "# 加載 YOLO 模型\n",
    "net = cv2.dnn.readNet(\"yolov4.weights\", \"yolov4.cfg\")\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "\n",
    "# 獲取未連接的輸出層的名稱\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# 載入類別名稱\n",
    "with open(yolo_names_path, \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "def detect_person(image):\n",
    "    height, width, channels = image.shape\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    \n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5 and classes[class_id] == \"person\":  # 檢測到人的類別\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                \n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                \n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    \n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    if isinstance(indexes, (np.ndarray, list)) and len(indexes) > 0:\n",
    "        if isinstance(indexes[0], (np.ndarray, list)):\n",
    "            x, y, w, h = boxes[indexes[0][0]]\n",
    "        else:\n",
    "            x, y, w, h = boxes[indexes[0]]\n",
    "        return x, y, w, h\n",
    "    return None\n",
    "\n",
    "def dance_video_processing(video_path= r'dance_video/dancer.mp4', showBG=True):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video stream or file\")\n",
    "        return\n",
    "\n",
    "    fps_time = 0\n",
    "    keypoints_list = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret_val, image = cap.read()\n",
    "        if not ret_val:\n",
    "            break\n",
    "\n",
    "        person_box = detect_person(image)\n",
    "        if person_box:\n",
    "            x, y, w, h = person_box\n",
    "            image = image[y:y+h, x:x+w]\n",
    "\n",
    "        results = pose_resnet50_mpii.keypoint_detection(images=[image], use_gpu=False)\n",
    "\n",
    "        if not showBG:\n",
    "            image = np.zeros(image.shape)\n",
    "\n",
    "        if results and len(results) > 0:\n",
    "            keypoints = results[0]['data']\n",
    "            keypoints_list.append(keypoints)\n",
    "            for key, (x, y) in keypoints.items():\n",
    "                if x > 0 and y > 0:\n",
    "                    cv2.circle(image, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "                    cv2.putText(image, f'{key}', (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "        # To display fps\n",
    "        cv2.putText(image, \"FPS: %f\" % (1.0 / (time.time() - fps_time)), (10, 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        # To display image\n",
    "        cv2.imshow('Dancer', image)\n",
    "        fps_time = time.time()\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return keypoints_list\n",
    "\n",
    "\n",
    "# 示例使用\n",
    "video_path = r'dance_video/breaking.mp4'\n",
    "keypoints = dance_video_processing(video_path)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
